<!DOCTYPE html>
<html>
    <head><meta name="generator" content="Hexo 3.9.0">
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
        <title>Dive into Deep</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="/css/bootstrap.min.css" rel="stylesheet">
        <link href="/css/bootstrap-material-design.css" rel="stylesheet">
        <link href="/css/ripples.css" rel="stylesheet">
        <link href="/css/index.css" rel="stylesheet"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    </head>
    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- 导航栏 -->
        <div class="navbar navbar-default">
            <div class="container-fluid">
                <div class="navbar-header">
                    <a class="navbar-brand" href="/">Dive into Deep</a>
                </div>
                <div class="navbar-collapse collapse navbar-responsive-collapse">
                    <form class="navbar-form navbar-right">
                        <div class="form-group">
                            <input type="text" class="form-control col-sm-8" placeholder="Search">
                        </div>
                    </form>
                    <ul class="nav navbar-nav navbar-right">
                        <li><a href="/archives">归档</a></li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 内容 -->
        <div class="col-md-8 col-md-offset-1">
    <h3>理解 Back-propagation</h3>
    <p>Back-propagation is a key procedure in deep learning. It’s used to calculate the gradients in the layers, which are then used in optimization processes (gradient descent and other derived algorithms). So it’s important to get a solid understanding on how back-propagation works. It really took me a long time to grasp the idea and finally derive the procedure.</p>
<p>What I’m writing here is mostly inspired by <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">Chapter 2 of the online book Neural Network and Deep Learning</a>. Honestly, I just read through Chapter 2, tried to understand it and then re-write the deriviation.</p>
<p>And my next step is to code back-propagation by hand, so that I could be more confident about it.</p>
<a id="more"></a>
<h1 id="Symbols-used-in-this-post"><a href="#Symbols-used-in-this-post" class="headerlink" title="Symbols used in this post"></a>Symbols used in this post</h1><p>$a^l_j$: Activation of neuron $j$ in layer $l$</p>
<p>$w^l_{jk}$: Weight of the connection between neuron $k$ in layer $l-1$ and neuron $j$ in layer $l$</p>
<p>$b^l_j$: Bias of neuron $j$ in layer $l$</p>
<p>$\sigma$: Activation function</p>
<p>We have the following formula according to the definition of neural network:</p>
<span>$a^l_j = \sigma(\sum_k w^l_{jk} a^{l-1}_k + b^l_j)$</span><!-- Has MathJax -->
<p>And to make our deriviation easier, define an intermediate variable $z^l_j$ to be the sum of weights and bias:</p>
<span>$$\begin{aligned}
z^l_j = \sum_k w^l_{jk} a^{l-1}_k + b^l_j
\end{aligned}$$</span><!-- Has MathJax -->
<p>so</p>
<p>$$<br>a^l_j = \sigma z^l_j<br>$$</p>
<p>And we use minimum squared error as the loss function (the loss function needs to satisfy two assumptions, which we just ignore here.):</p>
<p>$$<br>C = \dfrac{1}{2} \lVert \mathbf{y} - a^L(\mathbf{x}) \rVert^2<br>$$</p>
<p>Then we define the error of a layer, and it’s kind of the combination of how the weights and biases should change in a training pass. And the error is used in equations BP3 and BP4.</p>
<p>$$<br>\sigma^l_j = \dfrac{\partial C}{\partial z^l_j}<br>$$</p>
<h1 id="Back-propagation-equations"><a href="#Back-propagation-equations" class="headerlink" title="Back-propagation equations"></a>Back-propagation equations</h1><p>Here I just write down the four equations for backprob here, and later on I will present the proof of the formula.</p>
<span>$\delta^L_j = \dfrac{\partial C}{\partial a^L_j} \sigma&apos;(z^L_j) \label{BP1}\tag{BP1}$</span><!-- Has MathJax -->
<span>$\mathbf{\delta}^L = \bigtriangledown_a C \cdot \sigma&apos;(\mathbf{z}^L) \label{BP1a}\tag{BP1a}$</span><!-- Has MathJax -->
<span>$\delta^l_j = \dfrac{\partial C}{\partial z^l_j} = (\sum_k \delta^{l+1}_k w^{l+1}_{kj}) \cdot \sigma&apos; (z^l_j) \tag{BP2}$</span><!-- Has MathJax -->
<span>$\mathbf{\delta}^l = [ (\mathbf{W}^{l+1})^T \mathbf{\delta}^{l+1} ] \odot \sigma&apos; (\mathbf{z}^l) \tag{BP2a}$</span><!-- Has MathJax -->
<span>$\dfrac{\partial C}{\partial b^l_j} = \dfrac{\partial C}{\partial z^l_j} = \delta^L_j \tag{BP3}$</span><!-- Has MathJax -->
<span>$\dfrac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \tag{BP4}$</span><!-- Has MathJax -->
<h1 id="Deriviation-of-the-equations"><a href="#Deriviation-of-the-equations" class="headerlink" title="Deriviation of the equations"></a>Deriviation of the equations</h1><h2 id="BP1-and-BP1a"><a href="#BP1-and-BP1a" class="headerlink" title="BP1 and BP1a"></a>BP1 and BP1a</h2><p>Equations BP1 and BP1a calculate the error of the output layer. By using chain-rule of partial deriviation, we get:</p>
<span>$\delta^L_j = \sum_k \dfrac{\partial C}{\partial a^L_k} \dfrac{\partial a^L_k}{\partial z^L_j} = \dfrac{\partial C}{\partial a^L_j} \sigma&apos; (z^L_j)$</span><!-- Has MathJax -->
<p>BP1a is just the matrix form of BP1, but it gives us a more gloabal view:</p>
<span>$\mathbf{\delta}^L = \dfrac{\partial C}{\partial \mathbf{z}} = \bigtriangledown_\mathbf{a} C \cdot \sigma&apos; (\mathbf{z}^L)$</span><!-- Has MathJax -->
<h2 id="BP2-and-BP2a"><a href="#BP2-and-BP2a" class="headerlink" title="BP2 and BP2a"></a>BP2 and BP2a</h2><p>Then the error for hidden layers. Here we calculate the partial deriviation of layer $l$ against that of layer $l+1$, since we need to back-propagate the error from higher layer to lower layer.</p>
<span>$\delta^l_j = \dfrac{\partial C}{\partial z^l_j} = \sum_k \dfrac{\partial C}{\partial z^{l+1}_k} \cdot \dfrac{\partial z^{l+1}_k}{\partial z^l_j} = \sum_k \delta^{l+1}_k \cdot \dfrac{\partial z^{l+1}_k}{\partial z^l_j}$</span><!-- Has MathJax -->
<p>because</p>
<span>$z^{l+1}_k = \sum_m w^{l+1}_{km} \cdot \sigma (z^l_m) + b^{l+1}_k$</span><!-- Has MathJax -->
<p>and it’s partial deriviation:</p>
<span>$\dfrac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \cdot \sigma&apos; (z^l_j)$</span><!-- Has MathJax -->
<p>so we get:</p>
<span>$\delta^l_j = \dfrac{\partial C}{\partial z^l_j} = (\sum_k \delta^{l+1}_k w^{l+1}_{kj}) \cdot \sigma&apos; (z^l_j)$</span><!-- Has MathJax -->
<p>Write it in matrix form:</p>
<span>$\mathbf{\delta}^l = [ (\mathbf{W}^{l+1})^T \mathbf{\delta}^{l+1} ] \odot \sigma&apos; (\mathbf{z}^l)$</span><!-- Has MathJax -->
<h2 id="BP3"><a href="#BP3" class="headerlink" title="BP3"></a>BP3</h2><span>$\dfrac{\partial C}{\partial b^l_j} = \sum_k \dfrac{\partial C}{\partial z^l_k} \cdot \dfrac{\partial z^l_k}{\partial b^l_j} = \dfrac{\partial C}{\partial z^l_j} = \sigma^l_j$</span><!-- Has MathJax -->
<p>It’s straight forward because</p>
<span>$z^l_m = \sum_n w^l_{mn} a^{l-1}_n + b^l_m$</span><!-- Has MathJax -->
<p>And the part $\dfrac{\partial z^l_k}{\partial b^l_j} = \dfrac{\partial C}{\partial z^l_j}$ is 1 only when $k = j$, otherwise it’s 0.</p>
<h2 id="BP4"><a href="#BP4" class="headerlink" title="BP4"></a>BP4</h2><p>Similarly,</p>
<span>$\dfrac{\partial C}{\partial w^l_{jk}} = \sum_m \dfrac{\partial C}{\partial z^l_m} \cdot \dfrac{\partial z^l_m}{\partial w^l_{jk}}$</span><!-- Has MathJax -->
<p>because</p>
<span>$z^l_m = \sum_n w^l_{mn} a^{l-1}_n + b^l_m$</span><!-- Has MathJax -->
<p>and when and only when m = j, n = k, the derivative is not 0, so here we get BP4:</p>
<span>$\dfrac{\partial C}{\partial w^l_{jk}} = \dfrac{\partial C}{\partial z^l_j} \cdot a^{l-1}_k = \sigma^l_j a^{l-1}_k$</span><!-- Has MathJax -->

</div>


        <!-- 右边栏 -->
<div class="col-md-2">
    <div class="panel panel-primary">
        <div class="panel-heading">
            <h3 class="panel-title">标签</h3>
        </div>

        <div class="panel-body">
            
            
                <a href="/tags/akka/"> akka </a>
            
                <a href="/tags/architecture/"> architecture </a>
            
                <a href="/tags/deeplearning/"> deeplearning </a>
            
                <a href="/tags/haskell/"> haskell </a>
            
                <a href="/tags/java/"> java </a>
            
                <a href="/tags/linux/"> linux </a>
            
                <a href="/tags/math/"> math </a>
            
                <a href="/tags/monad/"> monad </a>
            
                <a href="/tags/mxnet/"> mxnet </a>
            
                <a href="/tags/nginx/"> nginx </a>
            
                <a href="/tags/php/"> php </a>
            
                <a href="/tags/probability/"> probability </a>
            
                <a href="/tags/python/"> python </a>
            
                <a href="/tags/pytorch/"> pytorch </a>
            
                <a href="/tags/statistics/"> statistics </a>
            
        </div>
    </div>

    <!-- 创建 archive list -->
    

    <!-- 归档 -->
    <div class="panel panel-primary">
        <div class="panel-heading">
            <h3 class="panel-title">归档</h3>
        </div>
        <div class="panel-body archive-list">
            
                <a href="/archives/2020/01"> 2020 Jan </a>
            
                <a href="/archives/2017/11"> 2017 Nov </a>
            
                <a href="/archives/2017/09"> 2017 Sep </a>
            
                <a href="/archives/2017/06"> 2017 Jun </a>
            
                <a href="/archives/2017/02"> 2017 Feb </a>
            
                <a href="/archives/2016/11"> 2016 Nov </a>
            
                <a href="/archives/2016/10"> 2016 Oct </a>
            
                <a href="/archives/2015/03"> 2015 Mar </a>
            
                <a href="/archives/2014/10"> 2014 Oct </a>
            
                <a href="/archives/2013/07"> 2013 Jul </a>
            
                <a href="/archives/2013/06"> 2013 Jun </a>
            
                <a href="/archives/2013/05"> 2013 May </a>
            
                <a href="/archives/2009/01"> 2009 Jan </a>
            
        </div>
    </div>

    <!-- <div class="panel panel-primary">
        <div class="panel-heading">
            <h3 class="panel-title">最新文章</h3>
        </div>
        <div class="panel-body">
            <div>离散型概率分布</div>
            <div>概率论基础公式</div>
            <div>在Nginx上配置PHP</div>
        </div>
    </div> -->
</div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


    </body>
    <script src="/js/jquery-1.10.2.min.js"></script>
    <script src="/js/material.js"></script>
    <script src="/js/ripples.js"></script>
    <script type="text/javascript">
        $.material.init();
    </script>
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?2853d265112c92ccbe9cea1dcd4a4b69";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
</html>
